## Section 1: Databricks Lakehouse Platform

**Describe the relationship between the data lakehouse and the data warehouse.**
- Unified Architecture: 
- vast data handling and flexibility of a data lake

A data lakehouse is a unified architecture that combines elements of both data lakes and data warehouses. It aims to provide the vast data handling and flexibility of a data lake with the strong schema and performance optimizations of a data warehouse. This allows users to perform both large-scale data science and machine learning, as well as business intelligence and SQL analytics, all within the same system.

In contrast, a traditional data warehouse is optimized mainly for SQL queries and business intelligence analysis, supporting structured data with a fixed schema, and often struggles with the scale and diversity of data types that a data lake can handle.

The lakehouse architecture bridges these capabilities, allowing an organization to manage both structured and unstructured data in a single platform with high performance, governance, and reliability.



1. Identify the improvement in data quality in the data lakehouse over the data lake.
2. Compare and contrast silver and gold tables, which workloads will use a bronze table as a source, which workloads will use a gold table as a source.
3. What is located in the data plane versus the control plane and what resides in the customerâ€™s cloud account.
4. Differentiate between all-purpose clusters and jobs clusters.
5. Identify how cluster software is versioned using the Databricks Runtime.
6. Identify how clusters can be filtered to view those that are accessible by the user.
7. Describe how clusters are terminated and the impact of terminating a cluster.
8.  Identify a scenario in which restarting the cluster will be useful.
9.  Describe how to use multiple languages within the same notebook.
10. Identify how to run one notebook from within another notebook.
11. Identify how notebooks can be shared with others.
12. Describe how Databricks Repos enables CI/CD workflows in Databricks.
13. Identify Git operations available via Databricks Repos.
14. Identify limitations in Databricks Notebooks version control functionality relative to Repos.
